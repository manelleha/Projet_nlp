{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Global Demo & Model Comparison (Batch Evaluation)\n",
                "\n",
                "Ce notebook permet de charger les mod√®les et de les **tester massivement** sur le dataset de validation.\n",
                "\n",
                "**Fonctionnalit√©s :**\n",
                "1.  Chargement des mod√®les (Baseline + Fine-Tuned).\n",
                "2.  **Batch Test** : On prend 50 (ou plus) tweets au hasard dans la validation.\n",
                "3.  **Comparatif** : On affiche les scores de chacun et un tableau d√©taill√© des erreurs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import joblib\n",
                "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "from IPython.display import display, HTML\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Config\n",
                "VAL_PATH = \"../data/twitter_val_clean.csv\"\n",
                "BASELINE_PATH = \"../models/baseline/baseline_model.joblib\"\n",
                "FINETUNED_PATH = \"../models/bert_finetuned\"\n",
                "\n",
                "LABEL_MAP = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\", 3: \"Irrelevant\"}\n",
                "\n",
                "# Chargement Dataset\n",
                "print(\"‚è≥ Chargement Dataset Validation...\")\n",
                "val_df = pd.read_csv(VAL_PATH)\n",
                "print(f\"‚úÖ {len(val_df)} tweets disponibles.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chargement des Mod√®les"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Baseline\n",
                "print(\"‚è≥ Chargement Baseline...\")\n",
                "bl_clf = joblib.load(BASELINE_PATH)\n",
                "bl_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "bl_bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
                "bl_bert.eval()\n",
                "print(\"‚úÖ Baseline OK.\")\n",
                "\n",
                "# 2. Fine-Tuned\n",
                "print(\"‚è≥ Chargement Fine-Tuned...\")\n",
                "ft_tokenizer = AutoTokenizer.from_pretrained(FINETUNED_PATH)\n",
                "ft_model = AutoModelForSequenceClassification.from_pretrained(FINETUNED_PATH)\n",
                "ft_model.eval()\n",
                "print(\"‚úÖ Fine-Tuned OK.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_baseline_pred(texts):\n",
                "    # Batch encoding for speed\n",
                "    inputs = bl_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
                "    with torch.no_grad():\n",
                "        outputs = bl_bert(**inputs)\n",
                "    embs = outputs.last_hidden_state[:, 0, :].numpy()\n",
                "    preds = bl_clf.predict(embs)\n",
                "    return preds\n",
                "\n",
                "def get_finetuned_pred(texts):\n",
                "    inputs = ft_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
                "    with torch.no_grad():\n",
                "        outputs = ft_model(**inputs)\n",
                "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
                "    preds = torch.argmax(probs, dim=1).numpy()\n",
                "    return preds"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Ex√©cution du Test (Batch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choisissez le nombre de tweets √† tester\n",
                "N_TEST = 100 \n",
                "print(f\"üß™ Lancement du test sur {N_TEST} tweets al√©atoires...\")\n",
                "\n",
                "sample = val_df.sample(n=N_TEST, random_state=None).reset_index(drop=True)\n",
                "texts = sample[\"clean_text\"].astype(str).tolist()\n",
                "labels = sample[\"label\"].values\n",
                "\n",
                "# Pr√©dictions (Batch par Batch si n√©cessaire, ici N est petit donc tout d'un coup)\n",
                "print(\"   ... processing Baseline\")\n",
                "pred_bl = get_baseline_pred(texts)\n",
                "\n",
                "print(\"   ... processing Fine-Tuning\")\n",
                "pred_ft = get_finetuned_pred(texts)\n",
                "\n",
                "print(\"‚úÖ Termin√©.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. R√©sultats et Comparaison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "acc_bl = accuracy_score(labels, pred_bl)\n",
                "acc_ft = accuracy_score(labels, pred_ft)\n",
                "\n",
                "print(f\"üìä R√âSULTATS SUR {N_TEST} TWEETS :\")\n",
                "print(f\"-----------------------------------\")\n",
                "print(f\"üéØ Baseline Accuracy   : {acc_bl*100:.2f}%\")\n",
                "print(f\"üöÄ Fine-Tuned Accuracy : {acc_ft*100:.2f}%\")\n",
                "print(f\"-----------------------------------\")\n",
                "\n",
                "if acc_ft > acc_bl:\n",
                "    print(\"üèÜ Le Fine-Tuned gagne !\")\n",
                "elif acc_bl > acc_ft:\n",
                "    print(\"üèÜ La Baseline gagne ! (Surprise ?)\")\n",
                "else:\n",
                "    print(\"ü§ù √âgalit√© parfaite.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Analyse D√©tail√©e (Tableau)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cr√©ation d'un DataFrame pour voir les erreurs\n",
                "results = pd.DataFrame({\n",
                "    \"Tweet\": texts,\n",
                "    \"True\": [LABEL_MAP[l] for l in labels],\n",
                "    \"Baseline\": [LABEL_MAP[p] for p in pred_bl],\n",
                "    \"FineTuned\": [LABEL_MAP[p] for p in pred_ft],\n",
                "    \"BL_Correct\": labels == pred_bl,\n",
                "    \"FT_Correct\": labels == pred_ft\n",
                "})\n",
                "\n",
                "# Fonction de style pour colorier\n",
                "def highlight_vals(row):\n",
                "    styles = [''] * len(row)\n",
                "    # Baseline col index = 2\n",
                "    if row['Baseline'] == row['True']:\n",
                "        styles[2] = 'background-color: #d4edda; color: #155724' # Green\n",
                "    else:\n",
                "        styles[2] = 'background-color: #f8d7da; color: #721c24' # Red\n",
                "        \n",
                "    # FineTuned col index = 3\n",
                "    if row['FineTuned'] == row['True']:\n",
                "        styles[3] = 'background-color: #d4edda; color: #155724'\n",
                "    else:\n",
                "        styles[3] = 'background-color: #f8d7da; color: #721c24'\n",
                "    return styles\n",
                "\n",
                "print(\"Affichage des 20 premiers r√©sultats (Vert = Correct, Rouge = Erreur) :\")\n",
                "display(results.head(20).style.apply(highlight_vals, axis=1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exemples o√π les mod√®les ne sont pas d'accord"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "disagreement = results[results[\"Baseline\"] != results[\"FineTuned\"]]\n",
                "print(f\"Il y a {len(disagreement)} tweets o√π les mod√®les ne sont pas d'accord.\")\n",
                "display(disagreement.head(10).style.apply(highlight_vals, axis=1))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}