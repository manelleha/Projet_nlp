{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Global Demo & Model Comparison (All 3 Models)\n",
                "\n",
                "Ce notebook est la **Comparaison Ultime**.\n",
                "\n",
                "Il teste en parall√®le :\n",
                "1.  **Baseline** (Frozen BERT + LogReg)\n",
                "2.  **Fine-Tuned** (BERT entra√Æn√©)\n",
                "3.  **Few-Shot** (LLM Llama-3 via Groq) üÜï\n",
                "\n",
                "**Note** : Assurez-vous d'avoir votre cl√© API Groq (variable d'environnement `GROQ_API_KEY`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "import joblib\n",
                "import json\n",
                "import random\n",
                "import re\n",
                "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
                "from sklearn.metrics import accuracy_score\n",
                "from IPython.display import display, HTML\n",
                "from openai import OpenAI\n",
                "\n",
                "# Config\n",
                "VAL_PATH = \"../data/twitter_val_clean.csv\"\n",
                "TRAIN_PATH = \"../data/twitter_train_clean.csv\"\n",
                "BASELINE_PATH = \"../models/baseline/baseline_model.joblib\"\n",
                "FINETUNED_PATH = \"../models/bert_finetuned\"\n",
                "GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
                "\n",
                "LABEL_MAP = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\", 3: \"Irrelevant\"}\n",
                "INV_LABELS = {v: k for k, v in LABEL_MAP.items()}\n",
                "\n",
                "# Chargement Dataset Validation\n",
                "print(\"‚è≥ Chargement Dataset Validation...\")\n",
                "val_df = pd.read_csv(VAL_PATH)\n",
                "print(f\"‚úÖ {len(val_df)} tweets disponibles.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Chargement des Mod√®les Classiques"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Baseline\n",
                "print(\"‚è≥ Chargement Baseline...\")\n",
                "bl_clf = joblib.load(BASELINE_PATH)\n",
                "bl_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
                "bl_bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
                "bl_bert.eval()\n",
                "\n",
                "# 2. Fine-Tuned\n",
                "print(\"‚è≥ Chargement Fine-Tuned...\")\n",
                "ft_tokenizer = AutoTokenizer.from_pretrained(FINETUNED_PATH)\n",
                "ft_model = AutoModelForSequenceClassification.from_pretrained(FINETUNED_PATH)\n",
                "ft_model.eval()\n",
                "\n",
                "print(\"‚úÖ Mod√®les classiques charg√©s.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration Few-Shot (LLM)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Config de la cl√© API\n",
                "# os.environ[\"GROQ_API_KEY\"] = \"VOTRE_CLE_ICI\" # D√©commentez pour tester en local si besoin\n",
                "\n",
                "if \"GROQ_API_KEY\" not in os.environ:\n",
                "    print(\"‚ö†Ô∏è  ATTENTION : Pas de cl√© API. Le Few-Shot ne marchera pas.\")\n",
                "    print(\"D√©finissez la variable d'environnement GROQ_API_KEY.\")\n",
                "else:\n",
                "    print(\"‚úÖ Cl√© API d√©tect√©e.\")\n",
                "\n",
                "client = OpenAI(\n",
                "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
                "    base_url=\"https://api.groq.com/openai/v1\"\n",
                ")\n",
                "\n",
                "# Pr√©paration des exemples (K=1)\n",
                "train_df = pd.read_csv(TRAIN_PATH)\n",
                "fewshot_examples = []\n",
                "for lbl in sorted(train_df[\"label\"].unique()):\n",
                "    subset = train_df[train_df[\"label\"] == lbl].sample(n=1, random_state=42)\n",
                "    for _, row in subset.iterrows():\n",
                "        fewshot_examples.append((row[\"clean_text\"], LABEL_MAP[int(lbl)]))\n",
                "random.shuffle(fewshot_examples)\n",
                "print(\"‚úÖ Exemples Few-Shot charg√©s (K=1).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Fonctions de Pr√©diction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_baseline_pred(texts):\n",
                "    inputs = bl_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
                "    with torch.no_grad():\n",
                "        outputs = bl_bert(**inputs)\n",
                "    embs = outputs.last_hidden_state[:, 0, :].numpy()\n",
                "    return bl_clf.predict(embs)\n",
                "\n",
                "def get_finetuned_pred(texts):\n",
                "    inputs = ft_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
                "    with torch.no_grad():\n",
                "        outputs = ft_model(**inputs)\n",
                "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
                "    return torch.argmax(probs, dim=1).numpy()\n",
                "\n",
                "def get_fewshot_pred_single(text):\n",
                "    # Construit le prompt\n",
                "    prompt = \"You are a strict tweet sentiment classifier (Negative, Neutral, Positive, Irrelevant).\\n\"\n",
                "    prompt += \"Here are examples:\\n\"\n",
                "    for ex_txt, ex_lbl in fewshot_examples:\n",
                "        prompt += f'Tweet: \"{ex_txt}\"\\nLabel: {ex_lbl}\\n\\n'\n",
                "    prompt += f'Now classify:\\nTweet: \"{text}\"\\nReturn strictly JSON: {{\"label\": \"...\"}}'\n",
                "    \n",
                "    try:\n",
                "        resp = client.chat.completions.create(\n",
                "            model=GROQ_MODEL,\n",
                "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "            max_tokens=30, temperature=0.0\n",
                "        )\n",
                "        raw = resp.choices[0].message.content\n",
                "        # Parse simple\n",
                "        if \"Negative\" in raw: return 0\n",
                "        if \"Neutral\" in raw: return 1\n",
                "        if \"Positive\" in raw: return 2\n",
                "        if \"Irrelevant\" in raw: return 3\n",
                "        return 1 # Default\n",
                "    except Exception as e:\n",
                "        print(f\"LLM Error: {e}\")\n",
                "        return 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Test Comparatif sur N Tweets\n",
                "Attention : Le Few-Shot est plus lent (appels API)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "N_TEST = 20  # On reste raisonnable pour ne pas attendre trop\n",
                "\n",
                "sample = val_df.sample(n=N_TEST, random_state=None).reset_index(drop=True)\n",
                "texts = sample[\"clean_text\"].astype(str).tolist()\n",
                "labels = sample[\"label\"].values\n",
                "\n",
                "print(f\"üöÄ Test sur {N_TEST} tweets...\")\n",
                "\n",
                "p_bl = get_baseline_pred(texts)\n",
                "p_ft = get_finetuned_pred(texts)\n",
                "\n",
                "print(\"   ... appel LLM (Few-Shot)...\")\n",
                "p_fs = [get_fewshot_pred_single(t) for t in texts]\n",
                "\n",
                "print(\"‚úÖ Termin√©.\")\n",
                "\n",
                "# Calcul et Affichage des Accuracy\n",
                "acc_bl = accuracy_score(labels, p_bl)\n",
                "acc_ft = accuracy_score(labels, p_ft)\n",
                "acc_fs = accuracy_score(labels, p_fs)\n",
                "\n",
                "print(f\"\\nüìä Scores d'Accuracy sur {N_TEST} tweets al√©atoires :\")\n",
                "print(f\"--------------------------------------\")\n",
                "print(f\"ü§ñ Baseline   : {acc_bl*100:.2f}%\")\n",
                "print(f\"üöÄ Fine-Tuned : {acc_ft*100:.2f}%\")\n",
                "print(f\"üß† Few-Shot   : {acc_fs*100:.2f}%\")\n",
                "print(f\"--------------------------------------\")\n",
                "\n",
                "# Tableau R√©sultats\n",
                "results = pd.DataFrame({\n",
                "    \"Tweet\": texts,\n",
                "    \"True\": [LABEL_MAP[l] for l in labels],\n",
                "    \"Baseline\": [LABEL_MAP[l] for l in p_bl],\n",
                "    \"FineTuned\": [LABEL_MAP[l] for l in p_ft],\n",
                "    \"FewShot\": [LABEL_MAP[l] for l in p_fs]\n",
                "})\n",
                "\n",
                "def color_cells(row):\n",
                "    styles = [''] * len(row)\n",
                "    # Columns indices: True=1, Baseline=2, FineTuned=3, FewShot=4\n",
                "    truth = row['True']\n",
                "    \n",
                "    for i, col in enumerate(['Baseline', 'FineTuned', 'FewShot'], start=2):\n",
                "        if row[col] == truth:\n",
                "            styles[i] = 'background-color: #d4edda; color: #155724' # Valide\n",
                "        else:\n",
                "            styles[i] = 'background-color: #f8d7da; color: #721c24' # Erreur\n",
                "    return styles\n",
                "\n",
                "display(results.style.apply(color_cells, axis=1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Test Manuel (3 Mod√®les)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text_manual(text):\n",
                "    text = text.lower()\n",
                "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
                "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
                "\n",
                "user_text = input(\"Tweet √† tester (anglais) : \")\n",
                "\n",
                "if user_text:\n",
                "    clean = clean_text_manual(user_text)\n",
                "    print(f\"Texte : {clean}\")\n",
                "    \n",
                "    val_bl = get_baseline_pred([clean])[0]\n",
                "    val_ft = get_finetuned_pred([clean])[0]\n",
                "    val_fs = get_fewshot_pred_single(clean)\n",
                "    \n",
                "    html = f\"\"\"\n",
                "    <div style='padding:10px; border:1px solid #ccc; border-radius:8px;'>\n",
                "        <h3>üîÆ Pr√©dictions :</h3>\n",
                "        <ul>\n",
                "            <li>ü§ñ <b>Baseline :</b> {LABEL_MAP[val_bl]}</li>\n",
                "            <li>üöÄ <b>Fine-Tuned :</b> {LABEL_MAP[val_ft]}</li>\n",
                "            <li>üß† <b>Few-Shot (LLM) :</b> {LABEL_MAP[val_fs]}</li>\n",
                "        </ul>\n",
                "    </div>\n",
                "    \"\"\"\n",
                "    display(HTML(html))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}